# subtrees (clusters). Use 10 distinct colors to highlight the branches cor-
# responding to each of the 10 subtrees.
library(dendextend)
hc.complete <- hclust(dist(X, method = "manhattan"), method = "complete")
dend <- as.dendrogram(hc.complete)
dend <- color_branches(dend, k = 10)
plot(dend, main = "Hierarchical Clustering Dendrogram (Complete Linkage)")
abline(h = hc.complete$height[length(hc.complete$height) - 9], col = "red", lty = 2)
plot(dend, main = "Hierarchical Clustering Dendrogram (Complete Linkage)")
hc.complete$height
hc.complete$height[length(hc.complete$height) - 9]
a <- hc.complete$height[length(hc.complete$height) - 9]
abline(h = a, col = "red", lty = 2)
abline(h = a, lty = 2)
plot(dend, main = "Hierarchical Clustering Dendrogram (Complete Linkage)")
a <- hc.complete$height[length(hc.complete$height) - 9]
plot(dend, main = "Hierarchical Clustering Dendrogram (Complete Linkage)")
a <- hc.complete$height[length(hc.complete$height) - 9]
abline(h = a, col = "black", lty = 2)
# 2. Perform hierarchical clustering
# using the principal component score vectors that can explain about 50%
# of the total variance (created in the previous problem) with the Euclidean
# distance and complete linkage. Create a dendrogram and draw a horizon-
# tal line at the height that results in 10 subtrees (clusters). Use 10 distinct
# colors to highlight the branches corresponding to each of the 10 subtrees.
# (hint: You can use the dendextend package to customize the dendrogram).
# (see example3.png attached)
pc_scores <- pr.out$x[, 1:which(cum_pve >= 0.5)[1]]
hc.complete <- hclust(dist(pc_scores, method = "Euclidian"), method = "complete")
hc.complete <- hclust(dist(pc_scores, method = "Euclidean"), method = "complete")
hc.complete <- hclust(dist(pc_scores, method = "euclidean"), method = "complete")
dend <- as.dendrogram(hc.complete)
dend <- color_branches(dend, k = 10)
plot(dend, main = "Hierarchical Clustering Dendrogram (Complete Linkage)")
a <- hc.complete$height[length(hc.complete$height) - 9]
abline(h = a, col = "black", lty = 2)
## 8.1 Single proportions (one-sample proportion test)
# H0: p=p0 vs H1: p!=p0
help(prop.test)
prop.test(39,215,.15) # approximate test using normal dist
## 8.2 Two independent proportions (two-sample proportion test)
# H0: p1=p2 vs H1: p1!=p2
# range is -1 to 1
# CI => prop1 - prop2 (positive if ci is positive range)
lewitt.machin.success <- c(9,4)
lewitt.machin.total <- c(12,13)
prop.test(lewitt.machin.success,lewitt.machin.total)
prop.test(lewitt.machin.success,lewitt.machin.total, correct=F)
z=(9/12-4/13)/sqrt((13/25)*(12/25)*(1/12+1/13))
z;z^2
matrix(c(9,4,3,9),2) # second column = number of negative outcomes
lewitt.machin <- matrix(c(9,4,3,9),2)
fisher.test(lewitt.machin)
#  odds ratio = the estimate of (p1/(1-p1))/(p2/(1-p2))
chisq.test(lewitt.machin) # the same as two-sided prop.test
chisq.test(lewitt.machin,correct=F)
#  odds ratio = the estimate of (p1/(1-p1))/(p2/(1-p2))
help(chisq.test)
??caesar.shoe
library(ISwR)
caesar.shoe # caesarean section:yes or no
# caesar.shoe[1,1]=10
class(caesar.shoe)
caesar.shoe.yes <- caesar.shoe["Yes",]
caesar.shoe.total <- margin.table(caesar.shoe,2)
caesar.shoe.yes
caesar.shoe.total
prop.test(caesar.shoe.yes,caesar.shoe.total)
prop.trend.test(caesar.shoe.yes,caesar.shoe.total) # smaller p-value
caff.marital <- matrix(c(652,1537,598,242,36,46,38,21,218
,327,106,67), nrow=3,byrow=T) # caffeine consumption vs marital status
colnames(caff.marital) <- c("0","1-150","151-300",">300")
rownames(caff.marital) <- c("Married","Prev.married","Single")
caff.marital
res.chi<-chisq.test(caff.marital)
summary(res.chi)
ls(chisq.test(caff.marital))
expt=chisq.test(caff.marital)$expected
obsv=chisq.test(caff.marital)$observed
class(expt)
matplot(expt,type="b",lty=1,pch=1:4)
matlines(obsv,type="b",lty=2,pch=1:4)
# contribution of each cell
E <- chisq.test(caff.marital)$expected
O <- chisq.test(caff.marital)$observed
(O-E)^2/E
library(ISwR)
attach(juul)
chisq.test(tanner,sex)
### 11 Multiple regression
rm(list=ls())
## 11.1 Plotting multivariate data
library(ISwR)
windows(record=T)
head(cystfibr)
par(mex=0.5) # mex=character size expansion factor
pairs(cystfibr, gap=0, cex.labels=0.9)
# gap=distance between subplots, in margin lines.
# cex.labels=graphics parameters for the diagonal text panel
pairs(cystfibr,gap=1, cex.labels=1.5)
attach(cystfibr)
# multiple regression
names(cystfibr)
dim(cystfibr)
# gap=distance between subplots, in margin lines.
# cex.labels=graphics parameters for the diagonal text panel
pairs(cystfibr,gap=1, cex.labels=1.5)
dim(cystfibr)
(mod1=lm(pemax~age+sex+height+weight+bmp+fev1+rv+frc+tlc))
# --> only coefficients
summary(mod1) # --> detailed results
# why?
# correlation이 존재할 수 있다. (multicolinearity)
# type 3이기 떄문에
anova(mod1)
Type1.mod1=anova(mod1)$"Pr(>F)" # notice the use of " "
Type1.mod1
Type1.mod1=Type1.mod1[-10]
Type1.mod1
TSS1=sum(anova(mod1)$"Sum Sq");TSS1 # total SS
TSS2=sum((pemax-mean(pemax))^2);TSS2 # total SS
# mean test H0: mean=mu0 H1: mean>mu0
# true mean=mu1(>mu0) delta=mu1-mu0
# true difference 'delta'
# test statistic T=(xbar-mu0)/(s/sqrt(n))
# T ~ noncentral t-dist
# noncentrality parameter 'v'
# delta=true difference: mu1-mu0
# noncentrality parameter v=delta/(sigma/sqrt(n))
# v=delta/(sigma/sqrt(n)) --> true difference divided by se of mean
help(pt)
# non centrality t-distribution cdf
curve(pt(x,25,ncp=3), col = 1, from=-6, to=6) # x ranges from 0 to 6]
# central t-distribution
curve(pt(x,25,0),col = 2, from=-6,to=6,add=T)
lines(seq(0,6,0.1),pt(seq(0,6,0.1),25,ncp=0))
abline(v=qt(.975,25)) # upper end of acceptance region for two-sided 5% level
abline(h=0.975)
# non central t distribution
curve(pt(x,25,ncp=3), from=-6, to=6)
abline(v=c(qt(.975,25),qt(.025,25)))
abline(h=0)
curve(pt(x,25,ncp=0), from=0, to=20)
curve(pt(x,25,ncp=10), from=0, to=20, add=T)
# null is true (central t distribution)
curve(dt(x,25,ncp=0), from=-10, to=10,col=2)
abline(h=0)
abline(v = qt(.975,25))
abline(v = qt(.025, 25))
# non centrality parameter = 5
# if statistics value is below 0.025, or above 0.095 => reject null
# non centrality t distribution일때 한쪽으로 치우침
#power depends on the magnitude of the non centrality parameter
# how far the central is from the 0
curve(dt(x,25,ncp=5), from=-10, to=10, add=T,col=3)
pt(qt(.975,25),25,ncp=3)
pt(qt(.975,25),25)
1-pt(qt(.975,25),25,ncp=3) # power of the test
## 9.2 Two-sample problems
cn=power.t.test(delta=0.5, sd=2, sig.level = 0.01, power=0.8) # sample size
cn
power.t.test(n=450, delta=0.5, sd=2, sig.level = 0.01) # power
power.t.test(n=478, delta=0.5, sd=2, sig.level = 0.01) # power
# Chapter 6
library(foreign)
A=read.spss("https://stats.idre.ucla.edu/wp-content/uploads/2016/02/alpha.sav")
class(A)
B=as.data.frame(A)
head(B)
library(psych)
B.alpha=alpha(B) # to get the cronbach's alpha
B.alpha
B.cov=cov(B)
B.cov
cor(B)
# high correlation not necessaryily provides high alpha (also vice versa)
B.eigen=eigen(B.cov)$values
B.eigen
# represent several variables to fewer transformed variables
var.exp=B.eigen/sum(B.eigen)*100
var.exp
library(lavaan)
head(HolzingerSwineford1939)
help(HolzingerSwineford1939)
# specify factor models
# confirmatory factor analysis
cfa.0<-'
visual =~ x1 + x2 + x3
textual =~ x4 + x5 + x6
speed =~ x7 + x8 + x9
'
# sem and cfa do the same thing(default setting)
# lavaan은 default setting 없음
res.0<-sem(cfa.0,std.lv=1,data=HolzingerSwineford1939)
summary(res.0)
fitmeasures(res.0, c("chisq","pvalue","cfi","rmsea","srmr","gfi"))
library(semPlot)
semPaths(res.0,sizeMan=6, esize=2, asize=3, nCharNodes = 4,
edge.label.cex=1, rotation=2, "model", mar=c(8,10,8,12), exoVar=F,
thresholdSize=0, curve=2, curvature=0.7)
semPaths
efa=factanal(HolzingerSwineford1939[,7:15],3)
efa
library(psych)
library(lavaan)
library(semPlot)
alpha(HolzingerSwineford1939 [c("x1","x2","x3")])
alpha(HolzingerSwineford1939[c("x4","x5","x6")])
alpha(HolzingerSwineford1939[c("x7","x8","x9")])
cfa.1<-'
visual =~ x1 + x2 + x3
textual =~ x4 + x5 + x6
speed =~ x7 + x8 + x9
'
res.1<-cfa(cfa.1,data=HolzingerSwineford1939)
### Indirect Effect
Model <- '
y1 ~ x1 + x2 + x3
y2 ~ y` + x3'
setwd("C:/Users/USER/OneDrive/바탕 화면/혜원/Graduate School/SPRING 2025/AMS 572 Data Analysis/data")
dat=read.table("path_data1.txt",header=T)[,1:5]
head(dat)
library(lavaan) # latent variable analysis
# 1. specify path model
# character expression
Model.1 <- '
y1 ~ x1 + x2 + x3       # regression model for y1
y2 ~ y1                 # regression model for y2
'
Model.2 <- '
y1 ~ x1 + x2 + x3
y2 ~ y1
x1 ~~ x1
x2 ~~ x2
x3 ~~ x3
y1 ~~ y1
y2 ~~ y2
x1 ~~ x2
x1 ~~ x3
x2 ~~ x3
'
Model.reg <-'
y1 ~ x1 + x2 + x3
'
fit.reg <- sem(Model.reg,data=dat)
# 다른 package에도 sem이 있다.
summary(fit.reg)
# lavaan package default: MLE
# 이거랑 OLS estimator 다름
summary(lm(y1~x1 + x2 + x3, dat))
0.9536*2
0.9536^2*(200-4)/200
fit1 <- sem(Model.1,data=dat)   # path analysis
summary(fit1)
fit1.lavaan <- lavaan(Model.2,data=dat)   # path analysis
summary(fit1.lavaan)
fm1=fitMeasures(fit1,c("chisq","pvalue","gfi","srmr","rmsea"))
# Ground Fault Circuit Interrupter
# Standardized Root Mean Square Residual
# root mean square error of approximation
fm1
modindx.1<-modindices(fit1)
head(modindx.1[order(modindx.1$mi,decreasing=T),]) # ordered modification index
Model.2 <- '
# observed model
y1 ~ x1 + x2 + x3
y2 ~ y1 + x2                         # add y2~x2
'
fit2 <- sem(Model.2,data=dat)
summary(fit2)
fm2=fitMeasures(fit2,c("chisq","pvalue","gfi","srmr","rmsea"))
fm2
modindx.2=modindices(fit2)
head(modindx.2[order(modindx.2$mi,decreasing=T),])
# a = 30하고 밑에 모델에 x3*a하면 이해 못함
# 하지만 model.3.1에서는 이해함
Model.3 <- '
y1 ~ x1 + x2 + x3
y2 ~ y1 + x2
y2 ~ x1
'
help(paste0) # concatenate without space
Model.3.1 <- c(
paste0("y1 ~ x1 + x2 + x3*",a),
"y2 ~ y1 + x2",
"y2 ~ `x1"
) #위에 거를 더 많이 씀
fit3<-sem(Model.3,data=dat)
summary(fit3)
fm3=fitMeasures(fit3,c("chisq","pvalue","gfi","srmr","rmsea"))
fm3
library(semPlot)
help(semPaths)
semPaths(fit3,sizeMan=6, esize=2, asize=3, nCharNodes = 4,
rotation=2, "model", "est", mar=c(8,10,8,10), exoVar=T,
thresholdSize=0, curve=2, curvature=0.7,layout="tree2",
edge.label.cex=1,edge.label.position=0.5)
Model.4 <- '
y1 ~ x2 + x3
y2 ~ y1 + x2
y2 ~ x1
'
fit4 <- sem(Model.4, data=dat)
summary(fit4)
fm4=fitMeasures(fit4,c("chisq","pvalue","gfi","srmr","rmsea"))
fm4
library(lavaan)
PI.dat <- read.csv("path-Ingram.csv")
head(PI.dat)
PI.model <- '
Intent ~ Attitude + SubNorm + PBC
Behavior ~ Intent
Attitude ~~ SubNorm + PBC
SubNorm ~~ PBC
'
PI.result <- sem(model=PI.model,data=PI.dat)
summary(PI.result)
PI.model.reg <- '
Intent ~ Attitude + SubNorm + PBC
Behavior ~ Intent
'
PI.result.reg <- sem(model=PI.model.reg,data=PI.dat)
summary(PI.result.reg)
fitmeasures(PI.result,c("chisq","pvalue","gfi","cfi","srmr","rmsea"))
PI.mi=modindices(PI.result)
head(PI.mi[order(PI.mi$mi,decreasing=T),])
PI.model2<- '
Intent ~ Attitude + SubNorm + PBC
Behavior ~ Intent + PBC
Attitude ~~ SubNorm + PBC
SubNorm ~~ PBC
'
PI.result2 <- sem(model=PI.model2,data=PI.dat)
summary(PI.result2)
fitmeasures(PI.result2,c("chisq","pvalue","gfi","cfi","srmr","rmsea"))
PI.mi2=modindices(PI.result2)
head(PI.mi2[order(PI.mi2$mi,decreasing=T),])
semPaths(PI.result2,sizeMan=6, esize=2, asize=3, nCharNodes = 4,
rotation=2, "model", "est", mar=c(8,10,8,10), exoVar=T,
thresholdSize=0, curve=2, curvature=0.7,layout="tree2",
edge.label.cex=1,edge.label.position=0.3)
# lavaan::sem, sem::sem 다름
# Convert to sem model:
library(semPlot)
library(sem)
library(lavaan)
PI.sem <- semPlot::semSyntax(PI.result2, "sem") # lavaan model --> sem::sem model
PI.res.sem <- sem:::sem(PI.sem, data = PI.dat)
library(DiagrammeR)
sem::pathDiagram(PI.res.sem,edge.labels="values",ignore.double=FALSE,ignore.self=TRUE,
min.rank="Attitude, SubNorm, PBC",max.rank="Behavior",standardize=TRUE)
##########################################################################
## Run file "PROCESS MACRO for R.R" before running the following syntax
##########################################################################
source("C:/Users/cspar/Dropbox/2025 SEM Final Year/R program/PROCESS MACRO for R.R")
head(data.example)
result1<-process(data=data.example,y="Y",x="X",m="M",model=4,
seed=31216)
result2<-process(data=data.example,y="Y",x="X",m="M",w="W",model=7,
seed=31216)
dat.custom<-read.csv("custom.csv")
head(dat.custom)
result3<-process(data=dat.custom,y="Y",x="X",m=c("M1","M2"),model=4,
seed=31216)
result4<-process(data=dat.custom,y="Y",x="X",m=c("M1","M2"),model=6,
seed=31216)
process(dat.custom,y="Y",x="X",m=c("M1","M2"),w="age",z="gender",
bmatrix=c(1,1,0,1,1,1), wmatrix=c(1,0,0,0,0,1),wzmatrix=c(0,0,0,0,0,1),
seed=280417)
################
teams<-read.csv("teams.csv")
head(teams)
process(data=teams,y="perform",x="dysfunc",m="negtone",model=4,total=1,seed=12345)
process(data=teams,y="perform",x="dysfunc",m="negtone",w='negexp',model=14,
jn=1,plot=1,total=1,seed=12345)
teams.fl<-read.table("teams_floodlight.txt",header=T)
teams.fl
with(teams.fl,matplot(negexp,teams.fl[,c(2,6,7)],type="b",pch=1:3,col=1:3,lty=1:3,ylab="effect"))
abline(v=-0.2337,h=0,lty=1:2)
legend("topright",c("effect","LLCI","ULCI"),pch=1:3,col=1:3,lty=1:3)
axis(1,at=-0.2337)
teams.it<-read.table("teams_interaction.txt",header=T)
teams.it
plot(teams.it[1:3,1],teams.it[1:3,3],type="b",pch=1,col=1,lty=1,
xlab="negtone",ylab="perform",ylim=c(min(teams.it[,3]),max(teams.it[,3])),xaxt="n")
axis(1,at=c(teams.it[1,1],teams.it[2,1],teams.it[3,1]))
points(teams.it[4:6,1],teams.it[4:6,3],type="b",pch=2,col=2,lty=2)
points(teams.it[7:9,1],teams.it[7:9,3],type="b",pch=3,col=3,lty=3)
legend("topright",title="negexp",c("16th percentile","50th percentile","84th percentile"),pch=1:3,col=1:3,lty=1:3)
quantile(teams.it[,1],c(0.16,0.50,0.84))
quantile(teams.it[,2],c(0.16,0.50,0.84))
# R practice
class(teams)
summary(teams)
library(lavaan)
model4<-'
negtone ~ a*dysfunc                # define effect ‘a’
perform ~ c*dysfunc + b*negtone    # define effects ‘b’ and ‘c’
med := a*b                         # define mediation effect ‘med’
'
model4.result<-lavaan::sem(model4,data=teams) # specify package name 'lavaan'
summary(model4.result)
set.seed(12345)
model4.boot<-lavaan::sem(model4,data=teams,se="bootstrap",bootstrap=5000)
summary(model4.boot)
model4.bootCI<-parameterestimates(model4.boot)
model4.bootCI
model4.bootCI2<-parameterestimates(model4.boot,boot.ci.type="bca.simple")
model4.bootCI2
2134567890
###########
teams$toneexp=teams$negtone*teams$negexp
model.14<- '
negtone ~ a*dysfunc
perform ~ c*dysfunc + b*negtone + d*negexp + e*toneexp
med1 := a*b
med2 := a*e
'
model.14.result<-sem(model.14,data=teams)
# :=  define parameter
# ind as a new parameter defined by b*a
Model2 <- '
y1 ~ a*x3
y2 ~ b*y1 + cp*x3
ind := b*a
tot := ind + cp
' # name the coefficients as character (lm은 못함)
Res2 = sem(Model2, dat)
# indirect effect
Model <- '
y` ~ x1 + x2 + x3
y2 ~ y1 + x3'
Res = sem(Model, dat)
summary(Res)
Res3 = sem(Model2, se = "bootstrap", bootstrap=5000, data=dat)
help("parameterEstimates")
dat=read.table("path_data1.txt",header=T)[,1:5]
# indirect effect
Model <- '
y` ~ x1 + x2 + x3
y2 ~ y1 + x3'
Res = sem(Model, dat)
head(dat)
# :=  define parameter
# ind as a new parameter defined by b*a
Model2 <- '
y1 ~ a*x3
y2 ~ b*y1 + cp*x3
ind := b*a
tot := ind + cp
' # name the coefficients as character (lm은 못함)
Res2 = sem(Model2, dat)
# indirect effect
Model <- '
y1` ~ x1 + x2 + x3
y2 ~ y1 + x3
'
Res = sem(Model, dat)
Res = sem(Model, data=dat)
# indirect effect
Model <- '
y1` ~ x1 + x2 + x3
y2 ~ y1 + x3
'
library(lavaan)
# indirect effect
Model <- '
y1` ~ x1 + x2 + x3
y2 ~ y1 + x3
'
Res = sem(Model, data=dat)
summary(Res)
# :=  define parameter
# ind as a new parameter defined by b*a
Model2 <- '
y1 ~ a*x3
y2 ~ b*y1 + cp*x3
ind := b*a
tot := ind + cp
' # name the coefficients as character (lm은 못함)
library(rvest)
page <- read_html(url)
url <- "https://afhayes.com/introduction-to-mediation-moderation-and-conditional-process-analysis.html"
page <- read_html(url)
a_tags <- html_nodes(page, "a")
hrefs <- html_attr(a_tags, "href")
zip_links <- hrefs[grepl("\\.hayes2022data.zip$")]
zip_links <- hrefs[grepl("\\.hayes2022data.zip$", hrefs)]
temp_file <<- "C:/Users/USER/OneDrive/바탕 화면/혜원/Graduate School/SPRING 2025/AMS 572 Data Analysis"
download.file(zip_links[1], destfile = paste0(tempfile,"/data.zip", mode = "wb")
download.file(zip_links[1], destfile = paste0(tempfile,"/data.zip") mode = "wb")
download.file(zip_links[1], destfile = paste0(tempfile,"/data.zip"), mode = "wb")
download.file(zip_links[1], destfile = paste0(tempfile,"/data.zip"), mode = "wb")
download.file(zip_links[1], destfile = paste0(temp_file,"/data.zip"), mode = "wb")
temp_file <<- "C:/Users/USER/OneDrive/바탕 화면/혜원/Graduate School/SPRING 2025/AMS 572 Data Analysis/data.zip"
download.file(zip_links[1], destfile = temp_file, mode = "wb")
url <- "https://afhayes.com/introduction-to-mediation-moderation-and-conditional-process-analysis.html"
temp_file <- "C:/Users/USER/OneDrive/바탕 화면/혜원/Graduate School/SPRING 2025/AMS 572 Data Analysis"
dest_path <- file.path(temp_file, "hayes2022data.zip")
download.file(zip_links[1], destfile = dest_path, mode = "wb")
url <- "https://afhayes.com/introduction-to-mediation-moderation-and-conditional-process-analysis.html"
temp_file <- "C:/Users/USER/OneDrive/바탕 화면/혜원/Graduate School/SPRING 2025/AMS 572 Data Analysis"
dest_path <- file.path(temp_file, "hayes2022data.zip")
page <- read_html(url)
a_tags <- html_nodes(page, "a")
hrefs <- html_attr(a_tags, "href")
zip_links <- hrefs[grepl("\\.zip$", hrefs)]
zip_url <- zip_links[1]
zip_url
download.file(zip_url, destfile = dest_path, mode = "wb")
unzip(dest_path, exdir = file.path(temp_file, "unzipped"))
zip_links
